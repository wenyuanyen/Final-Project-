---
title: "kmeans_37810"
author: "Weidi Pan, Yuhei Koshino, Wen Yuan Yen"
date: "11/5/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
data(wine, package="rattle")
head(wine)
```

```{r}
#exclude the “Type” variable from your inputs, by using wine[-1]
data.train <- wine[-1]
```

###Pseudocode/Algorithm

1. randomly pick k centers from the data
2. assign each observation to its closest center/cluster based on euclidean distance
3. calculating the new mean of each cluster
4. repeat 2 and 3 until centers stop changing


```{r}
#we want to write a function takes input the training data and the number of clusters k

k_means_clus=function(train_data,k)
{
  centers=train_data[sample.int(nrow(train_data),k),]  #randomly pick k centers from the data
  center_diff=100   #initial stopping criterion
  index=rep(0,nrow(train_data))   #this a vector where we store center index for each observation
  niter=0  #number of iterations needed to reach the stopping criterion
  while(center_diff>=1e-10)
  {
    niter=niter+1
    old_centers=centers  #storing values for old centers
    
    for (i in 1:nrow(train_data))  #for each observation
    {
      min_dist=1e10       #initial minimum dist between observations and center
      for (j in 1:nrow(centers))    #for each center
      {
        dist_to_center=sum((centers[j,]-train_data[i,])^2) #calculating dist to center
        if (dist_to_center<=min_dist)
        {
          index[i]=j   #assign center index to observation i
          min_dist=dist_to_center    #updating minimum dist
        }
      }
    }
    
    for (i in 1:nrow(centers)) #for each center
    {
      centers[i,]=apply(train_data[index==i,],2,mean) #updating centers, mean applied over columns
    }
    center_diff=mean((old_centers-centers)^2)  #comparing the new center to the old center
  }
  return(index)
}
```

```{r}
set.seed(123)
index=k_means_clus(data.train,3)
```


```{r}
library(fpc)
plotcluster(data.train, index)
```


###Questions:

1. Do the clusters seem well-separated?

The clusters 1 and 2 seem somewhat separated, cluster 2 and 3 seem not separated.


2. Develop a method to quantify how well your algorithm???s clusters correspond to the three wine types.

```{r}
comparison=table(wine$Type, index)  #comparing index to the original wine type
comparison
```
```{r}
mis_classif_rate1 = (1+13+20+19)/nrow(wine)
mis_classif_rate1
```

We can see from above misclassification rate = 0.2978 without scaling for wine data.


###Now, repeat the exercise using scaled data

```{r}
set.seed(15)
data.train2 <- scale(wine[-1])
index2=k_means_clus(data.train2,3)
plotcluster(data.train2, index2)
```


###Questions:

1. Show how scaling affects the results of your clustering

Yes, aftering scaling the data, all the clusters seem well separated. Scaling the data centers and/or scales the columns of the matrix. After scaling, the kmeans algorithm converges much faster. 

For the wine dataset, the values for some variables, like alcohol and magnesium, much larger than the values for other variables, thus scaling helps for the wine dataset.


2. Develop a method to quantify how well your algorithm???s clusters correspond to the three wine types.

```{r}
comparison2=table(wine$Type, index2)  #comparing index to the original wine type
comparison2
```

```{r}
mis_classif_rate2 = (3+3)/nrow(wine)
mis_classif_rate2
```

We can see from above misclassification rate = 0.0337 with scaling for wine data.


###Finally, repeat both steps for the iris dataset, available with the command

```{r}
data(iris)
head(iris)
```

###First without scaling:
```{r}
set.seed(12)
data.train3 = iris[-5]
index3=k_means_clus(data.train3,3)
plotcluster(data.train3, index3)
```

1. Do the clusters seem well-separated?

The clusters seem well-separated, except clusters 2 and 3 are a bit close to each other


2. Develop a method to quantify how well your algorithm???s clusters correspond to the three wine types.

```{r}
comparison3=table(iris$Species, index3) 
comparison3
```

```{r}
mis_classif_rate3 = (14+3)/nrow(iris)
mis_classif_rate3
```

We can see from above misclassification rate = 0.1133 without scaling for iris data.


###With scaling:
```{r}
set.seed(13)
data.train4 = scale(iris[-5])
index4=k_means_clus(data.train4,3)
plotcluster(data.train4, index4)
```

1. Do the clusters seem well-separated?

The clusters seem well-separated, except clusters 1 and 3 are a bit close to each other


2. Develop a method to quantify how well your algorithm???s clusters correspond to the three wine types.

```{r}
comparison4=table(iris$Species, index4) 
comparison4
```


```{r}
mis_classif_rate4 = (13+8+1)/nrow(iris)
mis_classif_rate4
```

We can see from above misclassification rate = 0.1467 with scaling for iris data.


###Does k-means work well for classifying this dataset? Does scaling help?

The kmeans does seem to work well for classifying Iris dasaset, with misclassification rate = 0.1467 and 0.1133. However, scaling does not seem to improve the classification rate. This occurs probably because, for the iris dataset, all the measurements for four variables are units for lengths and width, and are within similar ranges of values, so we have no need to scale the data; however, for the wine dataset, the values for some variables, like alcohol and magnesium, much larger than the values for other variables, thus scaling helps for the wine dataset.


